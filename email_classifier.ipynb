{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "import pickle\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_path):\n",
    "    with open(file_path,'r',encoding='gb18030') as f:\n",
    "        text = f.read().splitlines()\n",
    "    flag = 0\n",
    "    content=[]\n",
    "    for item in text:\n",
    "        if item == '<TEXT>':\n",
    "            flag =1\n",
    "            continue\n",
    "        elif item == '</TEXT>':\n",
    "            flag =0\n",
    "            continue\n",
    "        else:\n",
    "            if flag ==1 :\n",
    "                content.append(item)\n",
    "    punc = '~`!#$%^&*()_+-=|\\';\":/.,?><~·！@#￥%……&*（）——：’；、。，“”？》《{}【】'\n",
    "    content = re.sub('[%s]'%punc, \"\",' '.join(content))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '/Users/bbbblink.q/1_Python/spam/email/corpus/train/Data/001/001'\n",
    "# read_text(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_word(text):\n",
    "    jieba_list = jieba.cut(text, cut_all=False)\n",
    "    jieba_txt = ' ' .join(jieba_list)\n",
    "    jieba_txt =jieba_txt.split( )\n",
    "    return jieba_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Counter(txt):\n",
    "    counter={}\n",
    "    for word in txt:\n",
    "        if word not in counter:\n",
    "            counter[word] = 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(x,y):\n",
    "    for k,v in x.items():\n",
    "        if k in y.keys():\n",
    "            y[k]+=v\n",
    "        else:\n",
    "            y[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预处理训练集\n",
    "spam=[]\n",
    "ham=[]\n",
    "i=0\n",
    "index_path='/Users/bbbblink.q/1_Python/spam/email/index.txt'\n",
    "with open (index_path,'r') as f:\n",
    "    text = f.readlines()\n",
    "    text = text[1: -5000]\n",
    "for item in text:\n",
    "    item = item.split()\n",
    "    label = item[0]\n",
    "#     print(label)\n",
    "    f_path = item[1]\n",
    "    f_path = f_path.replace(\"..\",\"/Users/bbbblink.q/1_Python/spam/email/corpus/train\")\n",
    "    if os.path.isfile(f_path):\n",
    "        content1 = read_text(f_path) \n",
    "        if label == 'spam':\n",
    "            spam.append(content1)\n",
    "        else:\n",
    "            ham.append(content1)\n",
    "    else:\n",
    "        print('loss file: {}'.format(f_path))\n",
    "        i=i+1\n",
    "        continue\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建训练集\n",
    "file = open('train_spam.pickle', 'wb')\n",
    "pickle.dump(spam, file)\n",
    "file.close()\n",
    "\n",
    "file = open('train_ham.pickle', 'wb')\n",
    "pickle.dump(ham, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7443700953693617\n",
      "0.2556299046306382\n"
     ]
    }
   ],
   "source": [
    "#计算先验概率\n",
    "with open('train_spam.pickle', 'rb') as file:\n",
    "    spam = pickle.load(file)\n",
    "with open('train_ham.pickle','rb') as file:\n",
    "    ham = pickle.load(file)\n",
    "\n",
    "c_spam = len(spam)\n",
    "c_ham = len(ham)\n",
    "total = c_spam + c_ham\n",
    "p_spam = c_spam / total\n",
    "p_ham = c_ham / total\n",
    "print(p_spam)\n",
    "print(p_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500 7000 7500 8000 8500 9000 9500 10000 10500 11000 11500 12000 12500 13000 13500 14000 14500 15000 15500 16000 16500 17000 17500 18000 18500 19000 19500 20000 20500 21000 21500 22000 22500 23000 23500 24000 24500 25000 25500 26000 26500 27000 27500 28000 28500 29000 29500 30000 30500 31000 31500 32000 32500 33000 "
     ]
    }
   ],
   "source": [
    "#构建counter_spam\n",
    "counter_s = defaultdict(int)\n",
    "i = 0\n",
    "with open('train_spam.pickle', 'rb') as file:\n",
    "    spam_text = pickle.load(file)\n",
    "for item in spam_text:\n",
    "    text_cut = cut_word(item)\n",
    "    b=Counter(text_cut)\n",
    "    merge_dict(b,counter_s)\n",
    "    i = i+1\n",
    "    if i % 500==0:\n",
    "        print(i,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据存入pickle\n",
    "file = open('counter_spam.pickle', 'wb')\n",
    "pickle.dump(counter_s, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500 7000 7500 8000 8500 9000 9500 10000 10500 11000 "
     ]
    }
   ],
   "source": [
    "#构建counter_ham\n",
    "counter_h = defaultdict(int)\n",
    "i = 0\n",
    "with open('train_ham.pickle', 'rb') as file:\n",
    "    spam_text = pickle.load(file)\n",
    "for item in spam_text:\n",
    "    text_cut = cut_word(item)\n",
    "    b=Counter(text_cut)\n",
    "    merge_dict(b,counter_h)\n",
    "    i = i+1\n",
    "    if i % 500==0:\n",
    "        print(i,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据存入pickle\n",
    "file = open('counter_ham.pickle', 'wb')\n",
    "pickle.dump(counter_h, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,item in enumerate(counter_h.items()):\n",
    "#     print(item,end=\"\")\n",
    "#     if i==100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_g/7hk997tj1hq1fnspjw938hcc0000gn/T/jieba.cache\n",
      "Loading model cost 0.685 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 1000 1500 2000 2500 3000 3500 4000 4500 5000 "
     ]
    }
   ],
   "source": [
    "#预处理测试集\n",
    "gold = []\n",
    "contents = []\n",
    "i=0\n",
    "index_path='/Users/bbbblink.q/1_Python/spam/email/index.txt'\n",
    "with open (index_path,'r') as f:\n",
    "    text = f.readlines()\n",
    "test= text[-5000:] \n",
    "for item in test:\n",
    "    item = item.split()\n",
    "    label = item[0]\n",
    "    f_path = item[1]\n",
    "    f_path = f_path.replace(\"..\",\"/Users/bbbblink.q/1_Python/spam/email/corpus/train\")\n",
    "    if os.path.isfile(f_path):\n",
    "        text=read_text(f_path)\n",
    "        text_cut = cut_word(text)\n",
    "        contents.append(text_cut)\n",
    "        if label == 'spam':\n",
    "            gold.append(1)\n",
    "        else:\n",
    "            gold.append(0)\n",
    "    else:\n",
    "        print('loss file: {}'.format(f_path))\n",
    "    i=i+1    \n",
    "    if i % 500 == 0:\n",
    "        print(i,end=\" \")\n",
    "    continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('test.pickle', 'wb')\n",
    "pickle.dump(contents, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始测试正确率\n",
    "def naive_bayes(content):\n",
    "    result = 0\n",
    "    p_x_spam = 0\n",
    "    p_x_ham = 0\n",
    "    for word in content:\n",
    "        if remove_stop:\n",
    "            if word in stop_list:\n",
    "                continue\n",
    "        if spam_c[word]== 0 or ham_c[word] == 0:\n",
    "            continue\n",
    "        p_x_spam = math.log((spam_c[word])/c_spam)\n",
    "        p_x_ham = math.log((ham_c[word])/c_ham)\n",
    "    p_spam_x = p_x_spam + math.log(p_spam)\n",
    "    p_ham_x = p_x_ham + math.log(p_ham)\n",
    "    if p_spam_x / p_ham_x < threthold:\n",
    "        result = 1  \n",
    "    else:\n",
    "        result = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_c={}\n",
    "ham_c={}\n",
    "with open('counter_spam.pickle', 'rb') as file:\n",
    "    spam_c = pickle.load(file)\n",
    "\n",
    "with open('counter_ham.pickle','rb') as file:\n",
    "    ham_c = pickle.load(file)\n",
    "\n",
    "with open('test.pickle','rb') as file:\n",
    "    contents = pickle.load(file)\n",
    "\n",
    "#构建停用词表\n",
    "stop_path1='/Users/bbbblink.q/1_Python/spam/email/corpus/stop_list/baidu_stopwords.txt'\n",
    "with open(stop_path1,'r') as f:\n",
    "    stop_list1 = f.read().splitlines()\n",
    "stop_path2='/Users/bbbblink.q/1_Python/spam/email/corpus/stop_list/scu_stopwords.txt'\n",
    "with open(stop_path2,'r') as f:\n",
    "    stop_list2 = f.read().splitlines()\n",
    "for item in stop_list2:\n",
    "    stop_list1.append(item)\n",
    "stop_list = stop_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result=[]\n",
    "remove_stop = True\n",
    "threthold = 1\n",
    "for item in contents:\n",
    "    test_result.append(naive_bayes(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9286\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(len(gold)):\n",
    "    if test_result[i] == gold[i]:\n",
    "        c = c + 1\n",
    "    else:\n",
    "        continue\n",
    "acc = c / len(gold)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
